#This is the single project pyproject.toml

[build-system]
requires = ["setuptools>=67", "wheel", "pip"]
build-backend = "setuptools.build_meta"

[project]
name = "gr00t"
version = "0.1.0"
requires-python = "==3.10.*"
# Mirror the main repo's baseline dependencies so install behaves the same.
# torch/torchvision/torchcodec are sourced from the PyTorch nightly cu130 index
# flash-attn is built from source for SM 13.0 (RTX 5090)
dependencies = [
    "albumentations==1.4.18",
    "av==15.0.0",
    "bitsandbytes",
    "diffusers==0.35.1",
    "dm-tree==0.1.9",
    "lmdb==1.7.5",
    "msgpack==1.1.0",
    "msgpack-numpy==0.4.8",
    "pandas==2.2.3",
    "peft==0.17.1",
    "setuptools>=67",
    "termcolor==3.2.0",
    "torch>=2.7.0",
    "torchvision>=0.22.0",
    "transformers==4.51.3",
    "tyro==0.9.17",
    "flash-attn>=2.7.4",
    "click==8.1.8",
    "datasets==3.6.0",
    "einops==0.8.1",
    "gymnasium==1.2.2",
    "matplotlib==3.10.1",
    "numpy==1.26.4",
    "omegaconf==2.3.0",
    "scipy==1.15.3",
    "torchcodec>=0.4.0",
    "wandb==0.23.0",
    "pyzmq==27.0.1",
    "deepspeed==0.17.6",
    "tensorrt-cu13",
    "onnx",
    "onnxscript",
    "cuda-python",
    "onnxruntime-gpu>=1.23.2",
]

[project.optional-dependencies]
dev = [
    "ruff",
    "ipython",
]
tensorrt = [
    "onnx>=1.20.0",
]

[tool.setuptools.packages.find]
where = ["."]
include = ["gr00t*"]

# ── uv configuration ──────────────────────────────────────────────────────────
# flash-attn is built from source without build isolation (needs torch in the env first).
# uv does a two-phase install: first torch + other deps, then flash-attn.
[tool.uv]
no-build-isolation-package = ["flash-attn"]
prerelease = "allow"
index-strategy = "unsafe-best-match"

# PyTorch nightly cu130 index for RTX 5090 (Blackwell / SM 13.0)
# Not explicit: pytorch-triton and other torch transitive deps also live here.
# We use index-strategy=unsafe-best-match to avoid PyTorch index shadowing PyPI packages.
[[tool.uv.index]]
name = "pytorch-nightly-cu130"
url = "https://download.pytorch.org/whl/nightly/cu130"

# NVIDIA PyPI for TensorRT packages
[[tool.uv.index]]
name = "nvidia"
url = "https://pypi.nvidia.com"
explicit = true

# Route packages to their respective indexes / sources
[tool.uv.sources]
torch = { index = "pytorch-nightly-cu130" }
torchvision = { index = "pytorch-nightly-cu130" }
torchcodec = { index = "pytorch-nightly-cu130" }
flash-attn = { git = "https://github.com/Dao-AILab/flash-attention.git", tag = "v2.7.4.post1" }
tensorrt-cu13 = { index = "nvidia" }

# Set CUDA arch + parallelism for building flash-attn from source for SM 13.0
[tool.uv.extra-build-variables]
flash-attn = { TORCH_CUDA_ARCH_LIST = "13.0", MAX_JOBS = "8" }

[tool.ruff]
line-length = 100
target-version = "py310"
src = ["gr00t"]
exclude = [
    "__pycache__",
    ".git",
    ".mypy_cache",
    ".pytest_cache",
    ".vscode",
    ".venv",
    "dist",
    "logs",
    "*.ipynb",
    "gr00t/model/modules/nvidia/Eagle-Block2A-2B-v2",
    "external_dependencies",
]

[tool.ruff.format]
quote-style = "double"
indent-style = "space"
docstring-code-format = true

[tool.ruff.lint]
select = ["E", "F", "I"]
ignore = ["E501"]

[tool.ruff.lint.per-file-ignores]
"__init__.py" = ["F401"]

[tool.ruff.lint.isort]
case-sensitive = false
combine-as-imports = true
force-sort-within-sections = true
force-wrap-aliases = false
split-on-trailing-comma = false
lines-after-imports = 2
section-order = ["future", "standard-library", "third-party", "first-party", "local-folder"]